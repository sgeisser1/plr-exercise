0.04920946314476078
Train Epoch: 0 [0/60000 (0%)]	Loss: 2.277249
0.04920946314476078
Train Epoch: 0 [640/60000 (1%)]	Loss: 3.917344
0.04920946314476078
Train Epoch: 0 [1280/60000 (2%)]	Loss: 2.671318
0.04920946314476078
Train Epoch: 0 [1920/60000 (3%)]	Loss: 2.363581
0.04920946314476078
Train Epoch: 0 [2560/60000 (4%)]	Loss: 2.324564
0.04920946314476078
Train Epoch: 0 [3200/60000 (5%)]	Loss: 2.323221
0.04920946314476078
Train Epoch: 0 [3840/60000 (6%)]	Loss: 2.329215
0.04920946314476078
Train Epoch: 0 [4480/60000 (7%)]	Loss: 2.301671
0.04920946314476078
Train Epoch: 0 [5120/60000 (9%)]	Loss: 2.287691
0.04920946314476078
Train Epoch: 0 [5760/60000 (10%)]	Loss: 2.369611
0.04920946314476078
Train Epoch: 0 [6400/60000 (11%)]	Loss: 2.395314
0.04920946314476078
Train Epoch: 0 [7040/60000 (12%)]	Loss: 2.336502
0.04920946314476078
Train Epoch: 0 [7680/60000 (13%)]	Loss: 2.341395
0.04920946314476078
Train Epoch: 0 [8320/60000 (14%)]	Loss: 2.291780
0.04920946314476078
Train Epoch: 0 [8960/60000 (15%)]	Loss: 2.362418
0.04920946314476078
Train Epoch: 0 [9600/60000 (16%)]	Loss: 2.447446
0.04920946314476078
Train Epoch: 0 [10240/60000 (17%)]	Loss: 2.460514
0.04920946314476078
Train Epoch: 0 [10880/60000 (18%)]	Loss: 2.376162
0.04920946314476078
Train Epoch: 0 [11520/60000 (19%)]	Loss: 2.401275
0.04920946314476078
Train Epoch: 0 [12160/60000 (20%)]	Loss: 2.419211
0.04920946314476078
Train Epoch: 0 [12800/60000 (21%)]	Loss: 2.321413
0.04920946314476078
Train Epoch: 0 [13440/60000 (22%)]	Loss: 2.260558
0.04920946314476078
Train Epoch: 0 [14080/60000 (23%)]	Loss: 2.436603
0.04920946314476078
Train Epoch: 0 [14720/60000 (25%)]	Loss: 2.365048
0.04920946314476078
Train Epoch: 0 [15360/60000 (26%)]	Loss: 2.399963
0.04920946314476078
Train Epoch: 0 [16000/60000 (27%)]	Loss: 2.409466
0.04920946314476078
Train Epoch: 0 [16640/60000 (28%)]	Loss: 2.303765
0.04920946314476078
Train Epoch: 0 [17280/60000 (29%)]	Loss: 2.391011
0.04920946314476078
Train Epoch: 0 [17920/60000 (30%)]	Loss: 2.338376
0.04920946314476078
Train Epoch: 0 [18560/60000 (31%)]	Loss: 2.308378
0.04920946314476078
Train Epoch: 0 [19200/60000 (32%)]	Loss: 2.328192
0.04920946314476078
Train Epoch: 0 [19840/60000 (33%)]	Loss: 2.484761
0.04920946314476078
Train Epoch: 0 [20480/60000 (34%)]	Loss: 2.408018
0.04920946314476078
Train Epoch: 0 [21120/60000 (35%)]	Loss: 2.340462
0.04920946314476078
Train Epoch: 0 [21760/60000 (36%)]	Loss: 2.347531
0.04920946314476078
Train Epoch: 0 [22400/60000 (37%)]	Loss: 2.374099
0.04920946314476078
Train Epoch: 0 [23040/60000 (38%)]	Loss: 2.355608
0.04920946314476078
Train Epoch: 0 [23680/60000 (39%)]	Loss: 2.342852
0.04920946314476078
Train Epoch: 0 [24320/60000 (41%)]	Loss: 2.334638
0.04920946314476078
Train Epoch: 0 [24960/60000 (42%)]	Loss: 2.423372
0.04920946314476078
Train Epoch: 0 [25600/60000 (43%)]	Loss: 2.364893
0.04920946314476078
Train Epoch: 0 [26240/60000 (44%)]	Loss: 2.308115
0.04920946314476078
Train Epoch: 0 [26880/60000 (45%)]	Loss: 2.316032
0.04920946314476078
Train Epoch: 0 [27520/60000 (46%)]	Loss: 2.453817
0.04920946314476078
Train Epoch: 0 [28160/60000 (47%)]	Loss: 2.381269
0.04920946314476078
Train Epoch: 0 [28800/60000 (48%)]	Loss: 2.403144
0.04920946314476078
Train Epoch: 0 [29440/60000 (49%)]	Loss: 2.444631
0.04920946314476078
Train Epoch: 0 [30080/60000 (50%)]	Loss: 2.373025
0.04920946314476078
Train Epoch: 0 [30720/60000 (51%)]	Loss: 2.372469
0.04920946314476078
Train Epoch: 0 [31360/60000 (52%)]	Loss: 2.389582
0.04920946314476078
Train Epoch: 0 [32000/60000 (53%)]	Loss: 2.338037
0.04920946314476078
Train Epoch: 0 [32640/60000 (54%)]	Loss: 2.342557
0.04920946314476078
Train Epoch: 0 [33280/60000 (55%)]	Loss: 2.395243
0.04920946314476078
Train Epoch: 0 [33920/60000 (57%)]	Loss: 2.312513
0.04920946314476078
Train Epoch: 0 [34560/60000 (58%)]	Loss: 2.347594
0.04920946314476078
Train Epoch: 0 [35200/60000 (59%)]	Loss: 2.426371
0.04920946314476078
Train Epoch: 0 [35840/60000 (60%)]	Loss: 2.350783
0.04920946314476078
Train Epoch: 0 [36480/60000 (61%)]	Loss: 2.411541
0.04920946314476078
Train Epoch: 0 [37120/60000 (62%)]	Loss: 2.368845
0.04920946314476078
Train Epoch: 0 [37760/60000 (63%)]	Loss: 2.403889
0.04920946314476078
Train Epoch: 0 [38400/60000 (64%)]	Loss: 2.335965
0.04920946314476078
Train Epoch: 0 [39040/60000 (65%)]	Loss: 2.374930
0.04920946314476078
Train Epoch: 0 [39680/60000 (66%)]	Loss: 2.445677
0.04920946314476078
Train Epoch: 0 [40320/60000 (67%)]	Loss: 2.352000
0.04920946314476078
Train Epoch: 0 [40960/60000 (68%)]	Loss: 2.440299
0.04920946314476078
Train Epoch: 0 [41600/60000 (69%)]	Loss: 2.394762
0.04920946314476078
Train Epoch: 0 [42240/60000 (70%)]	Loss: 2.389803
0.04920946314476078
Train Epoch: 0 [42880/60000 (71%)]	Loss: 2.314827
0.04920946314476078
Train Epoch: 0 [43520/60000 (72%)]	Loss: 2.418761
0.04920946314476078
Train Epoch: 0 [44160/60000 (74%)]	Loss: 2.393875
0.04920946314476078
Train Epoch: 0 [44800/60000 (75%)]	Loss: 2.387512
0.04920946314476078
Train Epoch: 0 [45440/60000 (76%)]	Loss: 2.532834
0.04920946314476078
Train Epoch: 0 [46080/60000 (77%)]	Loss: 2.407278
0.04920946314476078
Train Epoch: 0 [46720/60000 (78%)]	Loss: 2.359328
0.04920946314476078
Train Epoch: 0 [47360/60000 (79%)]	Loss: 2.364316
0.04920946314476078
Train Epoch: 0 [48000/60000 (80%)]	Loss: 2.380636
0.04920946314476078
Train Epoch: 0 [48640/60000 (81%)]	Loss: 2.398834
0.04920946314476078
Train Epoch: 0 [49280/60000 (82%)]	Loss: 2.417598
0.04920946314476078
Train Epoch: 0 [49920/60000 (83%)]	Loss: 2.419626
0.04920946314476078
Train Epoch: 0 [50560/60000 (84%)]	Loss: 2.351536
0.04920946314476078
Train Epoch: 0 [51200/60000 (85%)]	Loss: 2.331560
0.04920946314476078
Train Epoch: 0 [51840/60000 (86%)]	Loss: 2.353887
0.04920946314476078
Train Epoch: 0 [52480/60000 (87%)]	Loss: 2.433154
0.04920946314476078
Train Epoch: 0 [53120/60000 (88%)]	Loss: 2.377483
0.04920946314476078
Train Epoch: 0 [53760/60000 (90%)]	Loss: 2.326378
0.04920946314476078
Train Epoch: 0 [54400/60000 (91%)]	Loss: 2.345815
0.04920946314476078
Train Epoch: 0 [55040/60000 (92%)]	Loss: 2.353450
0.04920946314476078
Train Epoch: 0 [55680/60000 (93%)]	Loss: 2.370713
0.04920946314476078
Train Epoch: 0 [56320/60000 (94%)]	Loss: 2.342010
0.04920946314476078
Train Epoch: 0 [56960/60000 (95%)]	Loss: 2.310733
0.04920946314476078
Train Epoch: 0 [57600/60000 (96%)]	Loss: 2.359831
0.04920946314476078
Train Epoch: 0 [58240/60000 (97%)]	Loss: 2.384453
0.04920946314476078
Train Epoch: 0 [58880/60000 (98%)]	Loss: 2.394608
0.04920946314476078
Train Epoch: 0 [59520/60000 (99%)]	Loss: 2.436665
Test set: Average loss: 2.4407, Accuracy: 1009/10000 (10%)
0.04920946314476078
Train Epoch: 1 [0/60000 (0%)]	Loss: 2.400701
0.04920946314476078
Train Epoch: 1 [640/60000 (1%)]	Loss: 2.455069
0.04920946314476078
Train Epoch: 1 [1280/60000 (2%)]	Loss: 2.383898
0.04920946314476078
Train Epoch: 1 [1920/60000 (3%)]	Loss: 2.414163
0.04920946314476078
Train Epoch: 1 [2560/60000 (4%)]	Loss: 2.396452
0.04920946314476078
Train Epoch: 1 [3200/60000 (5%)]	Loss: 2.461517
Trial 5 failed with parameters: {'learning_rate': 0.04920946314476078, 'epochs': 9} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 169, in <lambda>
    lambda trial: objective(trial, args, model, device, train_loader, test_loader, optimizer),
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 91, in objective
    train(args, model, device, train_loader, optimizer, epoch)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 28, in train
    optimizer.step()
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 290, in adam
    _, foreach = _default_to_fused_or_foreach(params, differentiable, use_fused=False)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in _default_to_fused_or_foreach
    foreach = not fused and all(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in <genexpr>
    foreach = not fused and all(
KeyboardInterrupt
Trial 5 failed with value None.
[33m[W 2024-03-05 13:48:35,590][39m Trial 5 failed with parameters: {'learning_rate': 0.04920946314476078, 'epochs': 9} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 169, in <lambda>
    lambda trial: objective(trial, args, model, device, train_loader, test_loader, optimizer),
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 91, in objective
    train(args, model, device, train_loader, optimizer, epoch)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 28, in train
    optimizer.step()
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 290, in adam
    _, foreach = _default_to_fused_or_foreach(params, differentiable, use_fused=False)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in _default_to_fused_or_foreach
    foreach = not fused and all(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in <genexpr>
    foreach = not fused and all(
KeyboardInterrupt
[33m[W 2024-03-05 13:48:35,593][39m Trial 5 failed with value None.
Traceback (most recent call last):
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 206, in <module>
    main()
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 168, in main
    study.optimize(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 169, in <lambda>
    lambda trial: objective(trial, args, model, device, train_loader, test_loader, optimizer),
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 91, in objective
    train(args, model, device, train_loader, optimizer, epoch)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 28, in train
    optimizer.step()
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 290, in adam
    _, foreach = _default_to_fused_or_foreach(params, differentiable, use_fused=False)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in _default_to_fused_or_foreach
    foreach = not fused and all(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in <genexpr>
    foreach = not fused and all(
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 206, in <module>
    main()
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 168, in main
    study.optimize(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/study.py", line 451, in optimize
    _optimize(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 66, in _optimize
    _optimize_sequential(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/home/sgeisser/.local/lib/python3.10/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 169, in <lambda>
    lambda trial: objective(trial, args, model, device, train_loader, test_loader, optimizer),
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 91, in objective
    train(args, model, device, train_loader, optimizer, epoch)
  File "/home/sgeisser/git/plr-exercise/scripts/train.py", line 28, in train
    optimizer.step()
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 166, in step
    adam(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/adam.py", line 290, in adam
    _, foreach = _default_to_fused_or_foreach(params, differentiable, use_fused=False)
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in _default_to_fused_or_foreach
    foreach = not fused and all(
  File "/home/sgeisser/.local/lib/python3.10/site-packages/torch/optim/optimizer.py", line 122, in <genexpr>
    foreach = not fused and all(
KeyboardInterrupt